{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "from model.attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.patch_embed import PatchEmbedding\n",
    "from model.transformer_layer import TransformerLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import argparse\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libGL.so.1: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/cv2/__init__.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _registerMatType\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mat_wrapper\n",
      "\u001b[0;31mImportError\u001b[0m: libGL.so.1: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_random_crop(image, crop_h, crop_w):\n",
    "    h, w = image.shape[:2]\n",
    "    max_x = w - crop_w\n",
    "    max_y = h - crop_h\n",
    "    \n",
    "    x = np.random.randint(0, max_x)\n",
    "    y = np.random.randint(0, max_y)\n",
    "    crop = image[y: y + crop_h, x: x + crop_w, :]\n",
    "    return crop\n",
    "\n",
    "\n",
    "def get_center_crop(image):\n",
    "    h, w = image.shape[:2]\n",
    "    if h > w:\n",
    "        return image[(h - w) // 2:-(h - w) // 2, :, :]\n",
    "    else:\n",
    "        return image[:, (w - h) // 2:-(w - h) // 2, :]\n",
    "\n",
    "\n",
    "class MnistDataset(Dataset):\n",
    "    r\"\"\"\n",
    "    Minimal image dataset where we take mnist images\n",
    "    add a texture background\n",
    "    change the color of the digit.\n",
    "    Model trained on this dataset is then required to predict the below 3 values\n",
    "    1. Class of texture\n",
    "    2. Class of number\n",
    "    3. R, G, B values (0-1) of the digit color\n",
    "    \"\"\"\n",
    "    def __init__(self, split, config, im_h=224, im_w=224):\n",
    "        self.split = split\n",
    "        self.db_root = config['root_dir']\n",
    "        self.im_h = im_h\n",
    "        self.im_w = im_w\n",
    "        \n",
    "        imdb = json.load(open(os.path.join(self.db_root,  'imdb.json')))\n",
    "        self.im_info = imdb['{}_data'.format(split)]\n",
    "        self.texture_to_idx = imdb['texture_classes_index']\n",
    "        self.idx_to_texture = {v:k for k,v in self.texture_to_idx.items()}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.im_info)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        entry = self.im_info[index]\n",
    "        digit_cls = int(entry['digit_name'])\n",
    "        digit_im = cv2.imread(os.path.join(self.db_root, entry['digit_image']))\n",
    "        digit_im = cv2.cvtColor(digit_im, cv2.COLOR_BGR2RGB)\n",
    "        digit_im = cv2.resize(digit_im, (self.im_h, self.im_w))\n",
    "        \n",
    "        # Discretize mnist images to be either 0 or 1\n",
    "        digit_im[digit_im > 50] = 255\n",
    "        digit_im[digit_im <= 50] = 0\n",
    "        mask_val = (digit_im > 0).astype(np.float32)\n",
    "        digit_im = np.concatenate((digit_im[:, :, 0][..., None] * float(entry['color_r']),\n",
    "                                   digit_im[:, :, 1][..., None] * float(entry['color_g']),\n",
    "                                   digit_im[:, :, 2][..., None] * float(entry['color_b'])), axis=-1)\n",
    "        im = cv2.imread(os.path.join(self.db_root, entry['texture_image']))\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "        if self.split == 'train':\n",
    "            im = get_random_crop(im, self.im_h, self.im_w)\n",
    "        else:\n",
    "            im = get_center_crop(im)\n",
    "            im = cv2.resize(im, (self.im_h, self.im_w))\n",
    "        out_im = mask_val * digit_im + (1 - mask_val) * im\n",
    "        im_tensor = torch.from_numpy(out_im).permute((2, 0, 1))\n",
    "        im_tensor = 2 * (im_tensor / 255) - 1\n",
    "        return {\n",
    "            \"image\" : im_tensor,\n",
    "            \"texture_cls\" : self.texture_to_idx[entry['texture_name']],\n",
    "            \"number_cls\" : digit_cls,\n",
    "            \"color\":torch.as_tensor([float(entry['color_r']),\n",
    "                                     float(entry['color_g']),\n",
    "                                      float(entry['color_b'])])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    r\"\"\"\n",
    "    Layer to take in the input image and do the following:\n",
    "        1.  Transform grid of image into a sequence of patches.\n",
    "            Number of patches are decided based on image height,width and\n",
    "            patch height, width.\n",
    "        2. Add cls token to the above created sequence of patches in the\n",
    "            first position\n",
    "        3. Add positional embedding to the above sequence(after adding cls)\n",
    "        4. Dropout if needed\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Example configuration\n",
    "        #   Image c,h,w : 3, 224, 224\n",
    "        #   Patch h,w : 16, 16\n",
    "        image_height = config['image_height']\n",
    "        image_width = config['image_width']\n",
    "        im_channels = config['im_channels']\n",
    "        emb_dim = config['emb_dim']\n",
    "        patch_embd_drop = config['patch_emb_drop']\n",
    "        \n",
    "        self.patch_height = config['patch_height']\n",
    "        self.patch_width = config['patch_width']\n",
    "        \n",
    "        # Compute number of patches for positional parameters initialization\n",
    "        #   num_patches = num_patches_h * num_patches_w\n",
    "        #   num_patches = 224/16 * 224/16\n",
    "        #   num_patches = 196\n",
    "        num_patches = (image_height // self.patch_height) * (image_width // self.patch_width)\n",
    "        \n",
    "        # This is the input dimension of the patch_embed layer\n",
    "        # After patchifying the 224, 224, 3 image will be\n",
    "        # num_patches x patch_h x patch_w x 3\n",
    "        # Which will be 196 x 16 x 16 x 3\n",
    "        # Hence patch dimension = 16 * 16 * 3\n",
    "        patch_dim = im_channels * self.patch_height * self.patch_width\n",
    "        \n",
    "        self.patch_embed = nn.Sequential(\n",
    "            # This pre and post layer norm speeds up convergence\n",
    "            # Comment them if you want pure vit implementation\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, emb_dim),\n",
    "            nn.LayerNorm(emb_dim)\n",
    "        )\n",
    "        \n",
    "        # Positional information needs to be added to cls as well so 1+num_patches\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, emb_dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(emb_dim))\n",
    "        self.patch_emb_dropout = nn.Dropout(patch_embd_drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # This is doing the B, 3, 224, 224 -> (B, num_patches, patch_dim) transformation\n",
    "        # B, 3, 224, 224 -> B, 3, 14*16, 14*16\n",
    "        # B, 3, 14*16, 14*16 -> B, 3, 14, 16, 14, 16\n",
    "        # B, 3, 14, 16, 14, 16 -> B, 14, 14, 16, 16, 3\n",
    "        #  B, 14*14, 16*16*3 - > B, num_patches, patch_dim\n",
    "        out = rearrange(x, 'b c (nh ph) (nw pw) -> b (nh nw) (ph pw c)',\n",
    "                      ph=self.patch_height,\n",
    "                      pw=self.patch_width)\n",
    "        out = self.patch_embed(out)\n",
    "        \n",
    "        # Add cls\n",
    "        cls_tokens = repeat(self.cls_token, 'd -> b 1 d', b=batch_size)\n",
    "        out = torch.cat((cls_tokens, out), dim=1)\n",
    "        \n",
    "        # Add position embedding and do dropout\n",
    "        out += self.pos_embed\n",
    "        out = self.patch_emb_dropout(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_heads = config['n_heads']\n",
    "        self.head_dim = config['head_dim']\n",
    "        self.emb_dim = config['emb_dim']\n",
    "        self.drop_prob = config['dropout'] if 'dropout' in config else 0.0\n",
    "        self.att_dim = self.n_heads * self.head_dim\n",
    "        \n",
    "        self.qkv_proj = nn.Linear(self.emb_dim, 3 * self.att_dim, bias=False)\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(self.att_dim, self.emb_dim),\n",
    "            nn.Dropout(self.drop_prob))\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(self.drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #  Converting to Attention Dimension\n",
    "        ######################################################\n",
    "        # Batch Size x Number of Patches x Dimension\n",
    "        B, N = x.shape[:2]\n",
    "        # Projecting to 3*att_dim and then splitting to get q, k v(each of att_dim)\n",
    "        # qkv -> Batch Size x Number of Patches x (3* Attention Dimension)\n",
    "        # q(as well as k and v) -> Batch Size x Number of Patches x Attention Dimension\n",
    "        q, k ,v = self.qkv_proj(x).split(self.att_dim, dim=-1)\n",
    "        # Batch Size x Number of Patches x Attention Dimension\n",
    "        # -> Batch Size x Number of Patches x (Heads * Head Dimension)\n",
    "        # -> Batch Size x Number of Patches x (Heads * Head Dimension)\n",
    "        # -> Batch Size x Heads x Number of Patches x Head Dimension\n",
    "        # -> B x H x N x Head Dimension\n",
    "        q = rearrange(q, 'b n (n_h h_dim) -> b n_h n h_dim',\n",
    "                      n_h=self.n_heads, h_dim=self.head_dim)\n",
    "        k = rearrange(k, 'b n (n_h h_dim) -> b n_h n h_dim',\n",
    "                      n_h=self.n_heads, h_dim=self.head_dim)\n",
    "        v = rearrange(v, 'b n (n_h h_dim) -> b n_h n h_dim',\n",
    "                      n_h=self.n_heads, h_dim=self.head_dim)\n",
    "        #########################################################\n",
    "        \n",
    "        # Compute Attention Weights\n",
    "        #########################################################\n",
    "        # B x H x N x Head Dimension @ B x H x Head Dimension x N\n",
    "        # -> B x H x N x N\n",
    "        att = torch.matmul(q, k.transpose(-2, -1)) * (self.head_dim**(-0.5))\n",
    "        att = torch.nn.functional.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        #########################################################\n",
    "        \n",
    "        # Weighted Value Computation\n",
    "        #########################################################\n",
    "        #  B x H x N x N @ B x H x N x Head Dimension\n",
    "        # -> B x H x N x Head Dimension\n",
    "        out = torch.matmul(att, v)\n",
    "        #########################################################\n",
    "        \n",
    "        # Converting to Transformer Dimension\n",
    "        #########################################################\n",
    "        # B x N x (Heads * Head Dimension) -> B x N x (Attention Dimension)\n",
    "        out = rearrange(out, 'b n_h n h_dim -> b n (n_h h_dim)')\n",
    "        #  B x N x Dimension\n",
    "        out = self.output_proj(out)\n",
    "        ##########################################################\n",
    "        \n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    r\"\"\"\n",
    "    Transformer block which is just doing the following\n",
    "        1. LayerNorm followed by Attention\n",
    "        2. LayerNorm followed by Feed forward Block\n",
    "        Both these also have residuals added to them\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        emb_dim = config['emb_dim']\n",
    "        ff_hidden_dim = config['ff_dim'] if 'ff_dim' in config else 4*emb_dim\n",
    "        ff_drop_prob = config['ff_drop'] if 'ff_drop' in config else 0.0\n",
    "        self.att_norm = nn.LayerNorm(emb_dim)\n",
    "        self.attn_block = Attention(config)\n",
    "        self.ff_norm = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        self.ff_block = nn.Sequential(\n",
    "            nn.Linear(emb_dim, ff_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(ff_drop_prob),\n",
    "            nn.Linear(ff_hidden_dim, emb_dim),\n",
    "            nn.Dropout(ff_drop_prob)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = out + self.attn_block(self.att_norm(out))\n",
    "        out = out + self.ff_block(self.ff_norm(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        n_layers = config['n_layers']\n",
    "        emb_dim = config['emb_dim']\n",
    "        num_classes = config['num_classes']\n",
    "        self.patch_embed_layer = PatchEmbedding(config)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(config) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "        self.fc_number = nn.Linear(emb_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Patchify and add CLS token\n",
    "        out = self.patch_embed_layer(x)\n",
    "        \n",
    "        # Go through the transformer layers\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        out = self.norm(out)\n",
    "        \n",
    "        # Compute logits\n",
    "        return self.fc_number(out[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_for_one_epoch(epoch_idx, model, mnist_loader, optimizer):\n",
    "    r\"\"\"\n",
    "    Method to run the training for one epoch.\n",
    "    :param epoch_idx: iteration number of current epoch\n",
    "    :param model: Transformer model\n",
    "    :param mnist_loader: Data loder for mnist\n",
    "    :param optimizer: optimizer to be used taken from config\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for data in tqdm(mnist_loader):\n",
    "        im = data['image'].float().to(device)\n",
    "        number_cls = data['number_cls'].long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        model_output = model(im)\n",
    "        loss = criterion(model_output, number_cls)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Finished epoch: {} | Number Loss : {:.4f}'.\n",
    "          format(epoch_idx + 1,\n",
    "                 np.mean(losses)))\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "# def train(args):\n",
    "#     #  Read the config file\n",
    "#     ######################################\n",
    "#     with open(args.config_path, 'r') as file:\n",
    "#         try:\n",
    "#             config = yaml.safe_load(file)\n",
    "#         except yaml.YAMLError as exc:\n",
    "#             print(exc)\n",
    "#     print(config)\n",
    "#     #######################################\n",
    "    \n",
    "#     # Set the desired seed value\n",
    "#     ######################################\n",
    "#     seed = config['train_params']['seed']\n",
    "#     torch.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "#     if device == 'cuda':\n",
    "#         torch.cuda.manual_seed_all(args.seed)\n",
    "#     #######################################\n",
    "    \n",
    "#     # Create the model and dataset\n",
    "#     model = VIT(config['model_params']).to(device)\n",
    "#     mnist = MnistDataset('train', config['dataset_params'],\n",
    "#                          im_h=config['model_params']['image_height'],\n",
    "#                          im_w=config['model_params']['image_width'])\n",
    "#     mnist_loader = DataLoader(mnist, batch_size=config['train_params']['batch_size'], shuffle=True, num_workers=4)\n",
    "#     num_epochs = config['train_params']['epochs']\n",
    "#     optimizer = Adam(model.parameters(), lr=config['train_params']['lr'])\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "#     # Create output directories\n",
    "#     if not os.path.exists(config['train_params']['task_name']):\n",
    "#         os.mkdir(config['train_params']['task_name'])\n",
    "    \n",
    "#     # Load checkpoint if found\n",
    "#     if os.path.exists(os.path.join(config['train_params']['task_name'],\n",
    "#                                    config['train_params']['ckpt_name'])):\n",
    "#         print('Loading checkpoint')\n",
    "#         model.load_state_dict(torch.load(os.path.join(config['train_params']['task_name'],\n",
    "#                                                       config['train_params']['ckpt_name']), map_location=device))\n",
    "#     best_loss = np.inf\n",
    "    \n",
    "#     for epoch_idx in range(num_epochs):\n",
    "#         mean_loss = train_for_one_epoch(epoch_idx, model, mnist_loader, optimizer)\n",
    "#         scheduler.step(mean_loss)\n",
    "#         # Simply update checkpoint if found better version\n",
    "#         if mean_loss < best_loss:\n",
    "#             print('Improved Loss to {:.4f} .... Saving Model'.format(mean_loss))\n",
    "#             torch.save(model.state_dict(), os.path.join(config['train_params']['task_name'],\n",
    "#                                                         config['train_params']['ckpt_name']))\n",
    "#             best_loss = mean_loss\n",
    "#         else:\n",
    "#             print('No Loss Improvement')\n",
    "\n",
    "def train():\n",
    "   \n",
    "\n",
    "    # Set the desired seed value\n",
    "    ######################################\n",
    "    config = {\n",
    "        'dataset_params': {\n",
    "            'root_dir': 'data'\n",
    "        },\n",
    "\n",
    "        'model_params': {\n",
    "            'n_heads': 8,\n",
    "            'head_dim': 64,\n",
    "            'emb_dim': 128,\n",
    "            'attn_drop': 0.1,\n",
    "            'ff_dim': 256,\n",
    "            'ff_drop': 0.1,\n",
    "            'n_layers': 6,\n",
    "            'bg_classes': 44,\n",
    "            'num_classes': 10,\n",
    "            'image_height': 224,\n",
    "            'image_width': 224,\n",
    "            'patch_height': 16,\n",
    "            'patch_width': 16,\n",
    "            'patch_emb_drop': 0.1,\n",
    "            'im_channels': 3\n",
    "        },\n",
    "\n",
    "        'train_params': {\n",
    "            'task_name': 'default',\n",
    "            'batch_size': 64,\n",
    "            'epochs': 100,\n",
    "            'lr': 0.001,\n",
    "            'seed': 1111,\n",
    "            'ckpt_name': 'vit_ckpt.pth'\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    seed = config['train_params']['seed']\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # if device == 'cuda':\n",
    "    #     torch.cuda.manual_seed_all(args.seed)\n",
    "    #######################################\n",
    "    \n",
    "    # Create the model and dataset\n",
    "    model = VIT(config['model_params']).to(device)\n",
    "    mnist = MnistDataset('train', config['dataset_params'],\n",
    "                         im_h=config['model_params']['image_height'],\n",
    "                         im_w=config['model_params']['image_width'])\n",
    "    mnist_loader = DataLoader(mnist, batch_size=config['train_params']['batch_size'], shuffle=True, num_workers=4)\n",
    "    num_epochs = config['train_params']['epochs']\n",
    "    optimizer = Adam(model.parameters(), lr=config['train_params']['lr'])\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    # Create output directories\n",
    "    if not os.path.exists(config['train_params']['task_name']):\n",
    "        os.mkdir(config['train_params']['task_name'])\n",
    "    \n",
    "    # Load checkpoint if found\n",
    "    if os.path.exists(os.path.join(config['train_params']['task_name'],\n",
    "                                   config['train_params']['ckpt_name'])):\n",
    "        print('Loading checkpoint')\n",
    "        model.load_state_dict(torch.load(os.path.join(config['train_params']['task_name'],\n",
    "                                                      config['train_params']['ckpt_name']), map_location=device))\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch_idx in range(num_epochs):\n",
    "        mean_loss = train_for_one_epoch(epoch_idx, model, mnist_loader, optimizer)\n",
    "        scheduler.step(mean_loss)\n",
    "        # Simply update checkpoint if found better version\n",
    "        if mean_loss < best_loss:\n",
    "            print('Improved Loss to {:.4f} .... Saving Model'.format(mean_loss))\n",
    "            torch.save(model.state_dict(), os.path.join(config['train_params']['task_name'],\n",
    "                                                        config['train_params']['ckpt_name']))\n",
    "            best_loss = mean_loss\n",
    "        else:\n",
    "            print('No Loss Improvement')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/938 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Caught error in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_113/1451137293.py\", line 48, in __getitem__\n    digit_im = cv2.cvtColor(digit_im, cv2.COLOR_BGR2RGB)\ncv2.error: OpenCV(4.5.3) /tmp/pip-req-build-f51eratu/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m best_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> 152\u001b[0m     mean_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_for_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmnist_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(mean_loss)\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# Simply update checkpoint if found better version\u001b[39;00m\n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36mtrain_for_one_epoch\u001b[0;34m(epoch_idx, model, mnist_loader, optimizer)\u001b[0m\n\u001b[1;32m     10\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m tqdm(mnist_loader):\n\u001b[1;32m     13\u001b[0m     im \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m     number_cls \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber_cls\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1212\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1238\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1238\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_utils.py:438\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31merror\u001b[0m: Caught error in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_113/1451137293.py\", line 48, in __getitem__\n    digit_im = cv2.cvtColor(digit_im, cv2.COLOR_BGR2RGB)\ncv2.error: OpenCV(4.5.3) /tmp/pip-req-build-f51eratu/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--config CONFIG_PATH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-e2a93ee7-91e8-4f0e-adfe-94472b39ded7.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description='Arguments for vit training')\n",
    "#     parser.add_argument('--config', dest='config_path',\n",
    "#                         default='config/default.yaml', type=str)\n",
    "#     args = parser.parse_args()\n",
    "#     train(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_accuracy(model, mnist_loader):\n",
    "    r\"\"\"\n",
    "    Method to get accuracy for number classification for trained model\n",
    "    :param model:\n",
    "    :param mnist_loader:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_total = 0.\n",
    "    num_correct = 0.\n",
    "    \n",
    "    for data in tqdm(mnist_loader):\n",
    "        im = data['image'].float().to(device)\n",
    "        number_cls = data['number_cls'].long().to(device)\n",
    "        model_output = model(im)\n",
    "        pred_num_cls_idx = torch.argmax(model_output, dim=-1)\n",
    "        num_total += pred_num_cls_idx.size(0)\n",
    "        num_correct += torch.sum(pred_num_cls_idx == number_cls).item()\n",
    "    num_accuracy = num_correct / num_total\n",
    "    print('Number Accuracy : {:2f}'.format(num_accuracy))\n",
    "\n",
    "   \n",
    "def visualize_pos_embed(model):\n",
    "    r\"\"\"\n",
    "    Method to save the positional embeddings cosine similarity map\n",
    "    Assumes number of patches to be 196\n",
    "    :param model:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # pos_embed = 1 x Num_patches+1 x D\n",
    "    # Get indexes after CLS\n",
    "    pos_emb = model.patch_embed_layer.pos_embed.detach().cpu()[0][1:]\n",
    "\n",
    "    plt.tight_layout(pad=0.1, rect=(0.1, 0.1, 0.9, 0.9))\n",
    "    fig, axs = plt.subplots(7, 7)\n",
    "    count = 0\n",
    "    for i in tqdm(range(196)):\n",
    "        row = i // 14\n",
    "        col = i % 14\n",
    "        if row % 2 == 0 and col % 2 == 0:\n",
    "            out = torch.cosine_similarity(pos_emb[i], pos_emb, dim=-1)\n",
    "            fig.add_subplot(7, 7, count+1)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            count += 1\n",
    "            plt.subplots_adjust(0.1, 0.1, 0.9, 0.9)\n",
    "            plt.imshow(out.reshape(14, 14), vmin=-1, vmax=1)\n",
    "    for idx, ax in enumerate(axs.flat):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "    if not os.path.exists('output'):\n",
    "        os.mkdir('output')\n",
    "    plt.savefig('output/position_plot.png', bbox_inches='tight')\n",
    "    \n",
    "    \n",
    "def visualize_attn_weights(mnist, model):\n",
    "    r\"\"\"\n",
    "    Uses trivial implementation of rollout.\n",
    "    :param mnist:\n",
    "    :param model:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_images = 10\n",
    "    idxs = torch.randint(0, len(mnist) - 1, (num_images,))\n",
    "    ims = torch.cat([mnist[idx]['image'][None, :] for idx in idxs]).float()\n",
    "    ims = ims.to(device)\n",
    "    attentions = []\n",
    "    \n",
    "    def get_attention(model, input, output):\n",
    "        attentions.append(output.detach().cpu())\n",
    "        \n",
    "    # Add forward hook\n",
    "    for name, module in model.named_modules():\n",
    "        if 'attn_dropout' in name:\n",
    "            module.register_forward_hook(get_attention)\n",
    "    \n",
    "    model(ims)\n",
    "    \n",
    "    # Handle residuals\n",
    "    attentions = [(torch.eye(att.size(-1)) + att)/(torch.eye(att.size(-1)) + att).sum(dim=-1).unsqueeze(-1) for att in attentions]\n",
    "    \n",
    "    result = torch.max(attentions[0], dim=1)[0]\n",
    "    # Max or mean both are fine\n",
    "    for i in range(1, 6):\n",
    "        att = torch.max(attentions[i], dim=1)[0]\n",
    "        result = torch.matmul(att, result)\n",
    "\n",
    "    masks = result\n",
    "    masks = masks[:, 0, 1:]\n",
    "    for i in range(num_images):\n",
    "        im_input = torch.permute(ims[i].detach().cpu(), (1, 2, 0)).numpy()\n",
    "        im_input = im_input[:, :, [2, 1, 0]]\n",
    "        im_input = (im_input+1)/2 * 255\n",
    "        mask = masks[i].reshape((14, 14)).numpy()\n",
    "        \n",
    "        mask = mask/np.max(mask)\n",
    "        \n",
    "        mask = cv2.resize(mask, (224, 224), interpolation=cv2.INTER_LINEAR)[..., None]\n",
    "        if not os.path.exists('output'):\n",
    "            os.mkdir('output')\n",
    "        cv2.imwrite('output/input_{}.png'.format(i), im_input)\n",
    "        cv2.imwrite('output/overlay_{}.png'.format(i), im_input*mask)\n",
    "\n",
    "\n",
    "def inference(args):\n",
    "    # Read the config file\n",
    "    ######################################\n",
    "    with open(args.config_path, 'r') as file:\n",
    "        try:\n",
    "            config = yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    print(config)\n",
    "    #######################################\n",
    "    \n",
    "    # Create the model and dataset\n",
    "    model = VIT(config['model_params']).to(device)\n",
    "    model.eval()\n",
    "    mnist = MnistDataset('test', config['dataset_params'],\n",
    "                         im_h=config['model_params']['image_height'],\n",
    "                         im_w=config['model_params']['image_width'])\n",
    "    mnist_loader = DataLoader(mnist, batch_size=config['train_params']['batch_size'], shuffle=True, num_workers=4)\n",
    "    \n",
    "    # Load checkpoint if found\n",
    "    if os.path.exists(os.path.join(config['train_params']['task_name'],\n",
    "                                   config['train_params']['ckpt_name'])):\n",
    "        print('Loading checkpoint')\n",
    "        model.load_state_dict(torch.load(os.path.join(config['train_params']['task_name'],\n",
    "                                                      config['train_params']['ckpt_name']), map_location=device))\n",
    "    else:\n",
    "        print('No checkpoint found at {}'.format(os.path.join(config['train_params']['task_name'],\n",
    "                                   config['train_params']['ckpt_name'])))\n",
    "    with torch.no_grad():\n",
    "        # Run inference and measure accuracy on number\n",
    "        get_accuracy(model, mnist_loader)\n",
    "        # Visualize positional embedding\n",
    "        visualize_pos_embed(model)\n",
    "        # Visualize attention weights\n",
    "        visualize_attn_weights(mnist, model)\n",
    "    \n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Arguments for vit training')\n",
    "parser.add_argument('--config', dest='config_path',\n",
    "                    default='config/default.yaml', type=str)\n",
    "args = parser.parse_args()\n",
    "inference(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
